{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Feasibility Study\n",
    "\n",
    "---\n",
    "\n",
    "This notebook performs the training, hyperparameter optimization, and evaluation of the feasibility neural network model for predicting the most comparison-efficient sorting algorithm.\n",
    "\n",
    "- The **most comparison-efficient sorting algorithm** for each sequence is determined, serving as the target label.  \n",
    "- As input features, the **presortedness metrics** are computed across the entire sequence for all five metrics under consideration.\n",
    "- The **number of comparisons required** is not taken into account in this setting, as the primary objective is to demonstrate that sorting algorithms can be predicted from presortedness metrics alone.  \n",
    "\n",
    "The **input file** is:  \n",
    "- the `dataset_sequences/dataset_sequences_200.pkl` containing the raw sequences.\n",
    "\n",
    "The **output files** computed by this notebook are:  \n",
    "- the `rq1_dataset/dataset_training_rq1_200.csv` containing presortedness features and labels.\n",
    "- the `rq1_model/model_rq1...` the best model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Installs the exact version of packages required for this notebook into the current Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.7.0\n",
    "%pip install pandas==2.2.3\n",
    "%pip install numpy==1.26.0\n",
    "%pip install tensorflow==2.15.0\n",
    "%pip install matplotlib==3.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for Training, Validation, and Test\n",
    "\n",
    "In order to train the model, it is necessary to determine the **most comparison-efficient sorting algorithm** (*Introsort*, *Insertionsort*, *Mergesort*, *Timsort*, *Quicksort*) for each sequence, which serves as the target label. As input features, we employ the **sampled presortedness metrics** (*Runs*, *Deletions*, *Inversions*, and *Inversion Distance*).  \n",
    "\n",
    "For the evaluation of the model, we additionally record the **number of comparisons required** to compute the presortedness metrics, as this reflects the computational overhead associated with feature extraction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from intellisorts_training_set import *\n",
    "\n",
    "dataset_path = 'dataset_sequences/dataset_sequences_200.pkl'\n",
    "\n",
    "print(f\"Loading sequences {dataset_path} ...\")\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset_200_dfs = pickle.load(f)\n",
    "\n",
    "print(\"Sequences loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ1 full array presortedness\n",
    "df_results = compute_training_data_extended(\n",
    "    dataset_dfs = dataset_200_dfs,\n",
    "    min_length = 200,\n",
    "    max_length = 200,\n",
    "    sampling_strategy = sampling_strategy_identity,\n",
    "    sample_size = 200\n",
    ")\n",
    "\n",
    "print(\"Dataset D200:\", len(df_results))\n",
    "df_results.to_csv('rq1_dataset/dataset_training_rq1_200.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Neural Network Model\n",
    "\n",
    "Model training including hyperparameter optimization in a grid search. Finally, shows an algorithm prediction summary that compares the **actual vs. predicted counts** for each sorting algorithm in the test set and reports the number of **true positives** per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from intellisorts_model_training import grid_search\n",
    "\n",
    "# load training dataset\n",
    "df_results = pd.read_csv('rq1_dataset/dataset_training_rq1_200.csv')\n",
    "\n",
    "# features\n",
    "train_input = df_results[['Inversions', 'Deletions', 'Runs', 'InversionDistance']]\n",
    "train_output = df_results['Algorithm']\n",
    "\n",
    "# perform grid search\n",
    "param_grid = {\n",
    "    'batch_size': [512],\n",
    "    'epochs': [500],\n",
    "    'layers': [0,1,2,3,4,5,6,7,8,9],\n",
    "    'layersize': [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "(\n",
    "    best_model,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    test_accuracy,\n",
    "    test_indices,\n",
    "    test_true_algorithms,\n",
    "    test_predicted_algorithms\n",
    ") = grid_search(\n",
    "    'rq1_model/model_rq1',\n",
    "    train_input,\n",
    "    train_output,\n",
    "    param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Comparisons Analysis\n",
    "\n",
    "This section evaluates the **average number of comparisons** required by:\n",
    "\n",
    "- The **minimum possible**, which represents the theoretical lower bound always choosing the most comparison efficient sorting algorithm\n",
    "- The **prediction model**\n",
    "- The classical sorting algorithms (Timsort, Merge Sort, Insertion Sort, Introsort, Quick Sort)\n",
    "\n",
    "The results are visualized on a logarithmic scale to account for big differences in magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_set_df = df_results.iloc[test_indices]\n",
    "test_set_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "c = []\n",
    "t = []\n",
    "\n",
    "for index, row in test_set_df.iterrows():\n",
    "    c.append(row[test_predicted_algorithms[index] + \"_Comparisons\"])\n",
    "    t.append(row[test_true_algorithms[index] + \"_Comparisons\"])\n",
    "\n",
    "minimum_possible = round(np.sum(t)/len(test_set_df), 1)\n",
    "algorithm_prediction_model = round(np.sum(c)/len(test_set_df), 1)\n",
    "\n",
    "def calculate_average(column_name):\n",
    "    return round(np.sum(test_set_df[column_name]) / len(test_set_df), 1)\n",
    "\n",
    "merge_sort_avg = calculate_average('Mergesort_Comparisons')\n",
    "timsort_avg = calculate_average('Timsort_Comparisons')\n",
    "introsort_avg = calculate_average('Introsort_Comparisons')\n",
    "quick_sort_avg = calculate_average('Quicksort_Comparisons')\n",
    "insertion_sort_avg = calculate_average('Insertionsort_Comparisons')\n",
    "\n",
    "data = {\n",
    "    'Algorithm': ['Optimal', 'Predicted','Timsort', 'Mergesort', 'Insertionsort', 'Introsort', 'Quicksort'],\n",
    "    'Average Value': [minimum_possible, round(algorithm_prediction_model, 1), timsort_avg, merge_sort_avg, insertion_sort_avg, introsort_avg, quick_sort_avg]\n",
    "}\n",
    "df_average_comp = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df_average_comp['Algorithm'], df_average_comp['Average Value'], color='skyblue')\n",
    "plt.title('Average Comparisons of Sorting Algorithms')\n",
    "\n",
    "for i, val in enumerate(df_average_comp['Average Value']):\n",
    "    plt.text(i, val + 100, str(val), ha='center')\n",
    "\n",
    "plt.ylabel('Average Comparisons (Logarithmic Scale)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure_rq1_avg_comparisons.svg\", format='svg')\n",
    "plt.show()\n",
    "\n",
    "print('Optimal: ', minimum_possible)\n",
    "print('Predicted: ', algorithm_prediction_model)\n",
    "print()\n",
    "print('Mergesort: ', merge_sort_avg)\n",
    "print('Timsort: ', timsort_avg)\n",
    "print('Introsort: ', introsort_avg)\n",
    "print('Quicksort: ', quick_sort_avg)\n",
    "print('Insertionsort: ', insertion_sort_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
