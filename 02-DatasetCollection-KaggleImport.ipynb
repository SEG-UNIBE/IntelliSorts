{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf377679-596d-4c5e-8f90-dc1290f071c9",
   "metadata": {},
   "source": [
    "# Dataset Collection: Kaggle Import\n",
    "---\n",
    "\n",
    "## Raw Dataset Import and Preprocessing\n",
    "\n",
    "Raw datasets from the `./kaggledatasets` directory are imported into Pandas DataFrames, with non-numeric columns and missing values removed. Each dataset is truncated to a maximum length.\n",
    "\n",
    "The cleaned collection is stored as:\n",
    "- Dataset `dataset_sequences_200.pkl` (min length: 200, max length: 200)\n",
    "    - A CSV view `dataset_sequences_200.csv` of the dataset\n",
    "- Dataset `dataset_sequences_10k.pkl` (min length: 1, max length: 10,000)\n",
    "    - A CSV view `dataset_sequences_10k.csv` of the dataset\n",
    "\n",
    "⚠️ **Note:** Since downloaded Kaggle datasets may vary over time, reproducing the *exact* same DataFrames is not guaranteed. The `dataset_sequences_200.pkl` and `dataset_sequences_10k.pkl` datasets are provided for reproducibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbab38-b4db-48c4-a35f-0ce123405541",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe02d0-e4fa-4421-96d7-203b3cdc2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas==2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed944d-6616-420c-b3b8-aae2c73d99be",
   "metadata": {},
   "source": [
    "## Kaggle Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5dca0-bfa4-4b97-8df4-3ec71c32cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = './kaggledatasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a5770-564b-416c-9af2-630e0f5a876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kaggle_import(dataset_directory):\n",
    "    print(\"Import kaggle data...\")\n",
    "    dataset_dfs = {}\n",
    "\n",
    "    # import files in the directory\n",
    "    for file_name in os.listdir(dataset_directory):\n",
    "        try:\n",
    "            dataset_name = os.path.splitext(file_name)[0]\n",
    "            dataset_dfs[dataset_name] = pd.read_csv(os.path.join(dataset_directory, file_name))\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error reading file '{file_name}': {e}\")\n",
    "            continue\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"UnicodeDecodeError reading file '{file_name}': {e}\")\n",
    "            continue\n",
    "        except pd.errors.EmptyDataError as e:\n",
    "            print(f\"EmptyDataError reading file '{file_name}': {e}\")\n",
    "            continue\n",
    "        except IsADirectoryError as e:\n",
    "            print(f\"Error reading file '{file_name}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"Amount of dataframes imported:\", len(dataset_dfs))\n",
    "\n",
    "    return dataset_dfs\n",
    "    \n",
    "def static_array_size(dataset_dfs, sequence_length):\n",
    "    print(\"Clean up kaggle data...\")\n",
    "    \n",
    "    #static array size\n",
    "    for key, df in dataset_dfs.items():\n",
    "        for column in df.columns:\n",
    "            if not df[column].apply(lambda x: isinstance(x, (int, float))).all() or len(df[column].values) < sequence_length:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "                \n",
    "        dataset_dfs[key] = df.dropna().head(sequence_length)\n",
    "\n",
    "def truncate_array_size(dataset_dfs, max_sequence_length):\n",
    "    print(\"Clean up kaggle data...\")\n",
    "    \n",
    "    #up to specific array size\n",
    "    for key, df in dataset_dfs.items():\n",
    "        for column in df.columns:\n",
    "            if not df[column].apply(lambda x: isinstance(x, (int, float))).all():\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "                \n",
    "        dataset_dfs[key] = df.dropna().head(max_sequence_length)\n",
    "\n",
    "def save_pickle(dataset_dfs, dataset_import):\n",
    "    # Save the dictionary to a file\n",
    "    with open(f'{dataset_import}.pkl', 'wb') as f:\n",
    "        pickle.dump(dataset_dfs, f)\n",
    "\n",
    "def save_csv(dataset_dfs, dataset_import):\n",
    "    print(\"Building CSV view...\")\n",
    "    rows = []\n",
    "    \n",
    "    for ds_name, df in dataset_dfs.items():\n",
    "        for col in df.columns:\n",
    "            series = df[col]\n",
    "            \n",
    "            # Row: dataset#column as source\n",
    "            row = {\"_source\": f\"{ds_name}#{col}\"}\n",
    "    \n",
    "            # Add length (number of valid entries)\n",
    "            row[\"_length\"] = len(series)\n",
    "    \n",
    "            # Fill values with column index naming (_0, _1, ...)\n",
    "            for i, val in enumerate(series):\n",
    "                row[f\"_{i}\"] = val\n",
    "    \n",
    "            rows.append(row)\n",
    "    \n",
    "    wide_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Sort by _length descending (longest original columns first)\n",
    "    wide_df.sort_values(by=\"_length\", ascending=False, inplace=True)\n",
    "    wide_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"Created CSV view: {len(wide_df)} rows, {wide_df.shape[1]} columns\")\n",
    "    wide_df.to_csv(f'{dataset_import}.csv', index=False)\n",
    "    print(f'Saved {dataset_import}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4265d5-811d-4507-9406-b6e4a4f9b997",
   "metadata": {},
   "source": [
    "## Create Dataset `dataset_sequences_200.pkl` (min length: 200, max length: 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d844f50-2dbe-463c-881c-8ccbc3675d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sequences_200 = kaggle_import(dataset_directory)\n",
    "static_array_size(dataset_sequences_200, 200)\n",
    "save_pickle(dataset_sequences_200, 'dataset_sequences_200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941879bf-e819-4ff0-b7f4-540c85c646de",
   "metadata": {},
   "source": [
    "## Import Dataset `dataset_sequences_10k.pkl` (min length: 1, max length: 10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d00e89-7ec2-4f8a-910a-0e2db56a9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sequences_10k = kaggle_import(dataset_directory)\n",
    "truncate_array_size(dataset_sequences_10k, 10000)\n",
    "save_pickle(dataset_sequences_10k, 'dataset_sequences_10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938253aa-58dd-405d-b107-c00d89501a95",
   "metadata": {},
   "source": [
    "## (Optional) Save as CSV View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913efa1-7868-4b28-b2d1-1803e2c0af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(dataset_sequences_200, 'dataset_sequences_200')\n",
    "save_csv(dataset_sequences_10k, 'dataset_sequences_10k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
