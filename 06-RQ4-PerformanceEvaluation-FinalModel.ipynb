{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training and Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook performs the training, hyperparameter optimization, and evaluation of the final neural network model for predicting the most comparison-efficient sorting algorithm.\n",
    "\n",
    "The **input file** is:  \n",
    "- the `dataset_sequences/dataset_sequences_10k.pkl` containing the raw sequences.\n",
    "\n",
    "The **output files** computed by this notebook are:  \n",
    "- the `rq4_dataset/dataset_training_rq4_400p.csv` containing presortedness features and labels.\n",
    "- the `rq4_model/model_rq4...` the **final model**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Installs the exact version of packages required for this notebook into the current Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.7.0\n",
    "%pip install pandas==2.2.3\n",
    "%pip install numpy==1.26.0\n",
    "%pip install tensorflow==2.15.0\n",
    "%pip install matplotlib==3.10.1\n",
    "%pip install seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets for Training, Validation, and Test\n",
    "\n",
    "In order to train the model, it is necessary to determine the **most comparison-efficient sorting algorithm** (*Insertionsort*, *Mergesort*, *Timsort*) for each sequence, which serves as the target label. As input features, we employ the **sampled presortedness metrics** (*Runs* and *Deletions*).  \n",
    "\n",
    "For the evaluation of the model, we additionally record the **number of comparisons required** to compute the presortedness metrics, as this reflects the computational overhead associated with feature extraction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from intellisorts_training_set import *\n",
    "\n",
    "dataset_path = 'dataset_sequences/dataset_sequences_10k.pkl'\n",
    "\n",
    "print(f\"Loading sequences {dataset_path} ...\")\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset_10k_dfs = pickle.load(f)\n",
    "\n",
    "print(\"Sequences loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ4 sampling for final model\n",
    "df_results = compute_training_data(\n",
    "    dataset_dfs = dataset_10k_dfs,\n",
    "    min_length = 400,\n",
    "    max_length = 10000,\n",
    "    sampling_strategy = sampling_strategy_hybrid,\n",
    "    sample_size = 40\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Dataset D400+:\", len(df_results))\n",
    "df_results.to_csv('rq4_dataset/dataset_training_rq4_400p.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Neural Network \n",
    "\n",
    "Model training including hyperparameter optimization in a grid search. Finally, shows an algorithm prediction summary that compares the **actual vs. predicted counts** for each sorting algorithm in the test set and reports the number of **true positives** per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from intellisorts_model_training import grid_search\n",
    "\n",
    "# load training dataset\n",
    "df_results = pd.read_csv('rq4_dataset/dataset_training_rq4_400p.csv')\n",
    "\n",
    "# features\n",
    "train_input = df_results[['Deletions', 'Runs','SequenceLength']]\n",
    "train_output = df_results['Algorithm']\n",
    "\n",
    "# perform grid search\n",
    "param_grid = {\n",
    "    'batch_size': [512],\n",
    "    'epochs': [500],\n",
    "    'layers': [0,1,2,3,4,5,6,7,8,9,10],\n",
    "    'layersize': [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "(\n",
    "    best_model,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    test_accuracy,\n",
    "    test_indices,\n",
    "    test_true_algorithms,\n",
    "    test_predicted_algorithms\n",
    ") = grid_search(\n",
    "    'rq4_model/model_rq4',\n",
    "    train_input,\n",
    "    train_output,\n",
    "    param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Misclassifications\n",
    "\n",
    "This section examines the model’s errors by comparing the **true labels** with the **predicted labels** for the test set.\n",
    "\n",
    "The figure visualizes, for each misclassified sequence, the number of comparisons associated with the true algorithm (left bar) and the predicted algorithm (right bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import rcParams, font_manager\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_scaled = scaler.transform(train_input)\n",
    "y_pred_all = best_model.predict(X_scaled)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time to predict the test set: \" + str(elapsed_time))\n",
    "\n",
    "y_pred_all_classes = np.argmax(y_pred_all, axis=1)\n",
    "predicted_algorithms_all = label_encoder.inverse_transform(y_pred_all_classes)\n",
    "\n",
    "df_results[\"y_pred\"] = predicted_algorithms_all\n",
    "\n",
    "test_set_df = df_results.iloc[test_indices]\n",
    "test_set_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"size of test set: \" + str(len(test_set_df)))\n",
    "\n",
    "missclassified_df = test_set_df[test_set_df[\"Algorithm\"] != test_set_df[\"y_pred\"]].copy()\n",
    "\n",
    "missclassified_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"number of missclassifications: \" + str(len(missclassified_df)))\n",
    "\n",
    "missclassified_df[\"f\"] = missclassified_df.apply(lambda row: row[row[\"y_pred\"] + \"_Comparisons\"], axis=1)\n",
    "missclassified_df[\"t\"] = missclassified_df.apply(lambda row: row[row[\"Algorithm\"] + \"_Comparisons\"], axis=1)\n",
    "\n",
    "missclassified_df[\"abs_diff\"] = missclassified_df[\"f\"] - missclassified_df[\"t\"]\n",
    "\n",
    "# Sort by absolute difference in descending order\n",
    "missclassified_df = missclassified_df.sort_values(by=\"abs_diff\", ascending=False).reset_index(drop=True)\n",
    "print(missclassified_df)\n",
    "\n",
    "unique_algorithms = list(set(missclassified_df[\"Algorithm\"].unique()) | set(missclassified_df[\"y_pred\"].unique()))\n",
    "palette = sns.color_palette(\"tab10\", len(unique_algorithms))\n",
    "color_map = dict(zip(unique_algorithms, palette))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(missclassified_df.index - 0.2, missclassified_df[\"t\"], width=0.4, \n",
    "       color=[color_map[alg] for alg in missclassified_df[\"Algorithm\"]])\n",
    "\n",
    "ax.bar(missclassified_df.index + 0.2, missclassified_df[\"f\"], width=0.4, \n",
    "       color=[color_map[alg] for alg in missclassified_df[\"y_pred\"]])\n",
    "\n",
    "ax.set_xticks(missclassified_df.index[::2])\n",
    "\n",
    "legend_labels = {\n",
    "    \"merge_sort\": \"Mergesort\",\n",
    "    \"insertion_sort\": \"Insertionsort\",\n",
    "    \"timsort\": \"Timsort\"\n",
    "}\n",
    "\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=color_map[alg], label=legend_labels.get(alg, alg))\n",
    "    for alg in unique_algorithms\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_patches, loc=\"upper right\")\n",
    "\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Comparisons\")\n",
    "\n",
    "plt.rc('font', size=24)  # only applies on a second run\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Predictions Compared to Timsort\n",
    "\n",
    "This section highlights the **largest improvements** achieved by the model relative to Timsort.\n",
    "The analysis computes, for each test instance, the absolute difference in the number of comparisons between **Timsort** and the algorithm predicted by the model. The 15 cases with the greatest improvements are selected for visualization.\n",
    "\n",
    "The figure presents a side-by-side comparison for each selected sequence:  \n",
    "- The **left bar** (blue) shows the number of comparisons required by Timsort.  \n",
    "- The **right bar** (colored by algorithm type) shows the number of comparisons required by the model’s predicted algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute difference between \"y_pred\" column and \"timsort\"\n",
    "test_set_df[\"difference\"] = np.abs(test_set_df[\"Timsort_Comparisons\"] - test_set_df.apply(lambda row: row[row[\"y_pred\"] + \"_Comparisons\"], axis=1))\n",
    "\n",
    "# Select the 20 rows with the largest difference\n",
    "test_df_15 = test_set_df.nlargest(15, \"difference\").copy()\n",
    "test_df_15.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df_15.drop(columns=[\"difference\"], inplace=True)\n",
    "\n",
    "test_df_15[\"f\"] = test_df_15.apply(lambda row: row[row[\"y_pred\"] + \"_Comparisons\"], axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(test_df_15.index - 0.2, test_df_15[\"Timsort_Comparisons\"], width=0.4, color=[color_map[\"Timsort\"]])\n",
    "\n",
    "ax.bar(test_df_15.index + 0.2, test_df_15[\"f\"], width=0.4, \n",
    "       color=[color_map[alg] for alg in test_df_15[\"y_pred\"]])\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=color_map[alg], label=legend_labels.get(alg, alg))\n",
    "    for alg in unique_algorithms]\n",
    "\n",
    "ax.legend(handles=legend_patches, loc=\"upper left\")\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Comparisons\")\n",
    "\n",
    "plt.rc('font', size=24)  # only applies on a second run\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Prediction Summary\n",
    "\n",
    "This section compares the **actual vs. predicted counts** for each sorting algorithm in the test set and reports the number of **true positives** per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_counts = test_set_df[\"Algorithm\"].value_counts()\n",
    "predicted_counts = test_set_df[\"y_pred\"].value_counts()\n",
    "\n",
    "correct_merge_count = ((test_set_df[\"y_pred\"] == \"Mergesort\") & (test_set_df[\"Algorithm\"] == \"Mergesort\")).sum()\n",
    "correct_insertion_count = ((test_set_df[\"y_pred\"] == \"Insertionsort\") & (test_set_df[\"Algorithm\"] == \"Insertionsort\")).sum()\n",
    "correct_timsort_count = ((test_set_df[\"y_pred\"] == \"Timsort\") & (test_set_df[\"Algorithm\"] == \"Timsort\")).sum()\n",
    "\n",
    "summary_table = pd.DataFrame({\n",
    "    \"Actual Count\": actual_counts,\n",
    "    \"Predicted Count\": predicted_counts\n",
    "})\n",
    "\n",
    "print(summary_table)\n",
    "print(\"true positive insertion_sort :\" + str(correct_insertion_count))\n",
    "print(\"true positive merge_sort :\" + str(correct_merge_count))\n",
    "print(\"true positive timsort :\" + str(correct_timsort_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Model Performance Relative to Timsort\n",
    "\n",
    "This figure places the model’s performance into context by comparing it with baseline and reference values:  \n",
    "\n",
    "- **Timsort** serves as the baseline and is normalized to 100%.  \n",
    "- **Best possible** represents the theoretical lower bound always choosing the most comparison efficient sorting algorithm.  \n",
    "- **Model** reflects the performance of the final neural network model.\n",
    "- The **overhead of presortedness calculation** is displayed separately as an orange bar stacked on top of the model’s prediction cost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timsort_value = 457\n",
    "prediction_model_value = 39\n",
    "presortedness_value = 15\n",
    "minimum_possible_value = 390\n",
    "\n",
    "# Compute relative percentages\n",
    "timsort_percentage = 100 \n",
    "prediction_percentage = (prediction_model_value / timsort_value) * 100\n",
    "presortedness_percentage = (presortedness_value / timsort_value) * 100\n",
    "minimum_percentage = (minimum_possible_value / timsort_value) * 100\n",
    "\n",
    "bar_labels = [\"Timsort\", \"Best Possible\", \"Model\"]\n",
    "main_bar_values = [timsort_percentage, minimum_percentage, prediction_percentage]\n",
    "presorted_bar_value = [0, 0, presortedness_percentage]\n",
    "\n",
    "colors_bottom = [\"none\", \"none\", \"orange\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "bars_main = ax.bar(bar_labels, main_bar_values)\n",
    "\n",
    "bars_bottom = ax.bar(bar_labels, presorted_bar_value, color=colors_bottom)\n",
    "\n",
    "ax.set_ylabel(\"Comparisons Relative to Timsort\")\n",
    "# ax.set_title(\"Sorting Algorithm Comparisons (Relative to Timsort = 100%)\")\n",
    "\n",
    "for bar, value in zip(bars_main, main_bar_values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, value + 2, f\"{value:.1f}%\", ha=\"center\", fontsize=24)\n",
    "\n",
    "ax.text(bars_bottom[2].get_x() + bars_bottom[2].get_width() / 2, presortedness_percentage + 1,  \n",
    "        f\"{presortedness_percentage:.2f}%\", ha=\"center\", fontsize=12, color=\"black\")\n",
    "\n",
    "legend_patch = mpatches.Patch(color=\"orange\", label=\"Presortedness Calculation\")\n",
    "ax.legend(handles=[legend_patch], loc=\"upper right\")\n",
    "ax.set_ylim(0, 130)\n",
    "ax.set_yticks(np.arange(0, 101, 20))\n",
    "\n",
    "plt.rc('font', size=20)  # only applies on a second run\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
