{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Feasibility Study\n",
    "\n",
    "---\n",
    "\n",
    "This notebook performs the training, hyperparameter optimization, and evaluation of the feasibility neural network model for predicting the most comparison-efficient sorting algorithm.\n",
    "\n",
    "The **input file** is `dataset_training_200.csv` (containing presortedness features and labels).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.7.0\n",
    "%pip install pandas==2.2.3\n",
    "%pip install numpy==1.26.0\n",
    "%pip install tensorflow==2.15.0\n",
    "%pip install matplotlib==3.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Neural Network Model\n",
    "\n",
    "1. Grid search\n",
    "2. Algorithm Prediction Summary: Compares the **actual vs. predicted counts** for each sorting algorithm in the test set and reports the number of **true positives** per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from intellisorts_model_training import grid_search\n",
    "\n",
    "# load training dataset\n",
    "df_results = pd.read_csv('dataset_training_200.csv')\n",
    "\n",
    "# features\n",
    "train_input = df_results[['Inversions', 'Deletions', 'Runs', 'Dis']]\n",
    "train_output = df_results['Algorithm']\n",
    "\n",
    "# perform grid search\n",
    "param_grid = {\n",
    "    'batch_size': [512],\n",
    "    'epochs': [500],\n",
    "    'layers': [0,1,2,3,4,5,6,7,8,9],\n",
    "    'layersize': [1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "(\n",
    "    best_model,\n",
    "    scaler,\n",
    "    label_encoder,\n",
    "    test_accuracy,\n",
    "    test_indices,\n",
    "    test_true_algorithms,\n",
    "    test_predicted_algorithms\n",
    ") = grid_search(\n",
    "    'model_rq1',\n",
    "    train_input,\n",
    "    train_output,\n",
    "    param_grid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Average Comparisons Analysis\n",
    "\n",
    "This section evaluates the **average number of comparisons** required by:\n",
    "\n",
    "- The **minimum possible**, which represents the theoretical lower bound always choosing the most comparison efficient sorting algorithm\n",
    "- The **prediction model**\n",
    "- The classical sorting algorithms (Timsort, Merge Sort, Insertion Sort, Introsort, Quick Sort)\n",
    "\n",
    "The results are visualized on a logarithmic scale to account for big differences in magnitude\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_set_df = df_results.iloc[test_indices]\n",
    "test_set_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "c = []\n",
    "t = []\n",
    "\n",
    "for index, row in test_set_df.iterrows():\n",
    "    c.append(row[test_predicted_algorithms[index]])\n",
    "    t.append(row[test_true_algorithms[index]])\n",
    "\n",
    "def calculate_average(column_name):\n",
    "    return round(np.sum(test_set_df[column_name]) / len(test_set_df), 1)\n",
    "\n",
    "minimum_possible = round(np.sum(t)/len(test_set_df), 1)\n",
    "algorithm_prediction_model = round(np.sum(c)/len(test_set_df), 1)\n",
    "\n",
    "merge_sort_avg = calculate_average('merge_sort')\n",
    "timsort_avg = calculate_average('timsort')\n",
    "introsort_avg = calculate_average('introsort')\n",
    "quick_sort_avg = calculate_average('quick_sort')\n",
    "insertion_sort_avg = calculate_average('insertion_sort')\n",
    "\n",
    "data = {\n",
    "    'Algorithm': ['Optimal', 'Predicted','Timsort', 'Mergesort', 'Insertionsort', 'Introsort', 'Quicksort'],\n",
    "    'Average Value': [minimum_possible, round(algorithm_prediction_model, 1), timsort_avg, merge_sort_avg, insertion_sort_avg, introsort_avg, quick_sort_avg]\n",
    "}\n",
    "df_average_comp = pd.DataFrame(data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(df_average_comp['Algorithm'], df_average_comp['Average Value'], color='skyblue')\n",
    "plt.title('Average Comparisons of Sorting Algorithms')\n",
    "\n",
    "for i, val in enumerate(df_average_comp['Average Value']):\n",
    "    plt.text(i, val + 100, str(val), ha='center')\n",
    "\n",
    "plt.ylabel('Average Comparisons (Logarithmic Scale)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figure_rq1_avg_comparisons.svg\", format='svg')\n",
    "plt.show()\n",
    "\n",
    "print('Optimal: ', minimum_possible)\n",
    "print('Predicted: ', algorithm_prediction_model)\n",
    "print()\n",
    "print('Mergesort: ', merge_sort_avg)\n",
    "print('Timsort: ', timsort_avg)\n",
    "print('Introsort: ', introsort_avg)\n",
    "print('Quicksort: ', quick_sort_avg)\n",
    "print('Insertionsort: ', insertion_sort_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
